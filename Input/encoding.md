# Encoding Code Decoding
> a. GPU Accelerartion
> b. Declair of REASONING
> c. Tokenizer & Ready to Answer
> d. Single Token Prediction

## a. GPU Acceleration
```
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
### GPU Optimization
1. Kernel Optimization
2. Operation Optimization
3. Model Compression
4. CUDA Tunning and Optimization

### 1. Kernel Optimization
CUDA Kernel Optimization that is GPU Calculation
>- fused Kernel
>- memory coalescing
>- theread/block optimization of batch

LLM Technic

#### a. Fused Kernel
Definition: Technology that combines multiple operations into one CUDA kernel (=fuse) and executes them at once.

*Typical Arithmetic Flow*
> 1. Kernel Launch
> 2. DATA LOAD by GPU
> 3. Arithmetic Execution
> 4. Save Results  
>
> So that, if has the multiple arithmetics run separate KERNELS.  

> Kernel1: x + b  
> Kernel2: ReLU(x)  
> Kernel3: Dropout(x)  

*Fused Kernel*
> Kernel1: (x + b -> ReLU -> Dropout)

In GPU, the most **EXPENSIVE** function is the `memory bandwidth`  
- Flash Attention also selected method that combining `QKáµ€ -> Softmax -> V` for Minimizing fused kernel.

Examples:   

| Fused Kernel    | ì„¤ëª…                             |  
| --------------- | ------------------------------ |  
| Fused LayerNorm | LayerNorm + Add + Bias ë“± í•©ì¹¨    |  
| Fused MLP       | Linear + GeLU + Dropout í•©ì¹¨     |  
| Fused Attention | Q,K,V ê³„ì‚° + softmax + matmul í•©ì¹¨ |  
| FlashAttention  | ì™„ì „í•œ Attention íŒ¨í„´ í“¨ì „        |  

#### b. Memory Coalescing
Definition: When Number of Threads read a memory, make memory `sequential` and `coalesce` them.

`WARP`: 32 threads
>- 1:1 Mapping among Threads-Index and Memory-Index 
>- row-reading
>- Minimize Stride
>- Pre-Transpose
>- shared-memory(Other Way to Reduce GPU-load)
>- Triton(Making Sequential Memory Address)
>- Pytorch AutoCoalescing

*1:1 Mapping among Threads-Index and Memor-Index*
```aiignore
int idx = threadIdx.x + blockDim.x * blockIdx.x;
output[idx] = input[idx];
```
â†’ thread0 â†’ input[0]  
â†’ thread1 â†’ input[1]  
â†’ thread2 â†’ input[2]  
â†’ â€¦

*row-reading*
PyTorch, Numpy: row-major

Good Example
```(coalescing O)
float val = A[row][threadIdx.x];  // í–‰ì—ì„œ ì—°ì†ëœ ì›ì†Œ ì½ê¸°
```
Bad Example
```(coalescing X)
float val = A[threadIdx.x][col];  // ì—´ì„ ë”°ë¼ ì ‘ê·¼ â†’ ì£¼ì†Œê°€ ëœ¬ê¸ˆì—†ì´ ë©€ì–´ì§
```

#### c. Thread/Block Optimization of Batch
Definition: Batch dimension(B) / what thread handle / what block bind / assign how many thread 

### 2. Operation Optimization
Change of Model Structure
>- Flash Attention
>- Fused MLP
>- LayerNorm fusion
>- QKV

Using Operation Library that Pytorch or NVIDIA

#### Flash Attetntion
Problem of Attention Model
```
Q @ K^T -> (b, heads, seq, seq)
```
If length of seq is too long, memory cannot hold it.

*The Idea of Flash Attention*
> Do not make Length of Attention to n^2
> Cut units of the tile, as soon as calculate and abandon it.


#### Fused MLP, LayerNorm
Normal MLP Sequence
```
Linear -> GELU -> Linear -> Dropout
```
in Kernel
```
Kernel Calling 1: Linear
Kernel Calling 2: GELU
Kernel Calling 3: Linear
Kernel Calling 4: Dropout
```

*Fused Kernel*
```Fused kernel
W1 @ x -> GELU -> W2 @ (Result) -> Dropout
```
- use one "CUDA KERNEL"
- Same Logic with LayerNorm

#### QKV Fusion (QKV Projection Fusion)
Basic of Attention
```
Q = x * Wq  
K = x * Wk  
V = x * Wv
```
Three times of Kernel Calling

*QKV Fusion*
```perl
W = [Wq | Wk | Wv]
```
```Arithmetic Flow
QKV = x @ W
```

### 3. Model Compression
Model Compression 
>- Quantization
>- Pruning
>- Distilation

Descend Amount of Caputation

#### Quantization
float(32) -> INT8 or INT4
> Post-Training Quantization(PTQ): Quantizing After Training
> Quantization-Aware Training(QAT): Qunatizing During Training 

#### Pruning
Get rid of Unnecessary Parameters

#### Distilation
Teacher - Student
> Student Imitate Teacher's Answer

### 4. CUDA Tunnig and Optimization
Optimization GPU Code on CUDA LEVEL
>- a. Memory Hierarchy Optimization
>- b. Thread / Warp / Block Mapping
>- c. Instruction-Level Optimization
>- d. Asynchronous Execution & Overlap
>- e. Precision & Tensor Core Optimization

#### a. Memory Hierarchy Optimization
Main Key of GPU Calculation is Memory, not Arithmetic.

*Main Strategy*
- Global Memory: Access Minimization
- Shared / Register: Maximum Usage 

*Main Methods*
- Coalesced Memory Access
- Shared Memory tiling
- Register blocking
- Avoid bank conflict
- Prefetching

ğŸ”‘ 70% of CUDA Optimization up to Memory.

#### b. Thread / Warp / Block Mapping
(Hardware-Friendly Parallelization)
Thread(32) â†’ Warp
a Number of Warp â†’ SM

*Main Strategy*
- Warp divergence Reduction
- Occupancy Maximization

*Main Methods*
- Branch ì œê±° (if â†’ mask)
- Thread-per-element Design
- Block size Tunning (128 / 256 / 512)
- Warp-specialization

#### c. Instruction-Level Optimization
Make Computing-Cost Cheaper

*Main Strategy*
- Reduction of Expensive Computing
- Pipelining

*Main Methods*
- FMA 
- fast math (__expf, __logf)
- Loop unrolling
- Instruction fusion

#### d. Asynchronous Execution & Overlap
*Main Strategy*  
- Arithmetic and Memory Overlap  

*Main Methods*
- CUDA Streams
- Async memcpy
- Double buffering
- Pipeline parallelism

#### e. Precision & Tensor Core Optimization
Hardware-Unit

(NVIDIA RESEARCHER MOST-NOTICED PARTS)

*Main Strategy*
- Usage of Tensor Core

*Main Methods*
- FP16/BF16
- INT8/INT4
- MMA Instruction
- Proper memory Aligning


## b. Declair of REASONING
```
model.eval()
```

## c. Tokenizer & Ready to Answer
```
import tiktoken # pip install tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
```
- OpenAI GPT-2ì™€ ë™ì¼í•œ BPE í† í¬ë‚˜ì´ì €
- ëª¨ë¸ì´ í•™ìŠµë  ë•Œ ì‚¬ìš©í•œ í† í° ë¶„í•  ë°©ì‹ê³¼ ë°˜ë“œì‹œ ê°™ì•„ì•¼ í•¨
ğŸ‘‰ í† í¬ë‚˜ì´ì € ë‹¤ë¥´ë©´ ì¶œë ¥ì€ ì „ë¶€ ì“°ë ˆê¸°

### Readiness
```
idx = torch.tensor(idx).unsqueeze(0).to(device)
```
| ì½”ë“œ                  | ì˜ë¯¸                           |
| ------------------- | ---------------------------- |
| `torch.tensor(idx)` | ë¦¬ìŠ¤íŠ¸ â†’ í…ì„œ                     |
| `unsqueeze(0)`      | batch ì°¨ì› ì¶”ê°€ â†’ `(1, seq_len)` |
| `.to(device)`       | CPU or GPU ì´ë™                |

ğŸ‘‰ ëª¨ë¸ ì…ë ¥ í˜•íƒœ = (batch, sequence)

## d. Single Token Prediction
```
with torch.no_grad():
    logits = model(idx)
```
- ì¶”ë¡ ì´ë¯€ë¡œ gradient ê³„ì‚° X  
- ì¶œë ¥ í˜•íƒœ:
```
(batch, seq_len, vocab_size)
```
---
```
logits = logits[:, -1, :]
```
- ë§ˆì§€ë§‰ í† í° ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ í† í° í™•ë¥ ë§Œ ì‚¬ìš©
- shape:
```
(1, vocab_size)
```
---
ğŸ” Top-10 í›„ë³´ ì¶œë ¥
```
top_logits, top_indices = torch.topk(logits, 10)
```
í™•ë¥ (ì •í™•íˆëŠ” logit)ì´ ê°€ì¥ ë†’ì€ í† í° 10ê°œ

---
```
for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):
    print(f"{p:.2f}\t {i}\t {tokenizer.decode([i])}")
```
- logit ê°’
- í† í° ID
- ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´

ğŸ‘‰ ëª¨ë¸ì´ â€œë‹¤ìŒì— ë‚˜ì˜¬ ê²ƒ ê°™ë‹¤â€ê³  ìƒê°í•˜ëŠ” ë‹¨ì–´ë“¤

---
```
idx_next = torch.argmax(logits, dim=-1, keepdim=True)
```
- greedy decoding  
- ê°€ì¥ ë†’ì€ í™•ë¥  í•˜ë‚˜ ì„ íƒ  
---

```
flat = idx_next.squeeze(0)
out = tokenizer.decode(flat.tolist())
print(out)
```
- í…ì„œ â†’ ë¬¸ìì—´
- â€œDobby is ___â€ ì˜ ___ì— ë“¤ì–´ê°ˆ ë‹¨ì–´

## e. 