# [CD] Multi-Head Attention
Transformerì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œì¸ â€œMulti-Head Attentionâ€ (ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜) ì„ PyTorchë¡œ ì§ì ‘ êµ¬í˜„í•œ í´ë˜ìŠ¤.

## ğŸ§© ì „ì²´ êµ¬ì¡° ìš”ì•½
```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out):  # ì´ˆê¸°í™”
        ...
    def forward(self, x):  # ìˆœì „íŒŒ
        ...
```
Transformerì—ì„œ ì…ë ¥ x (ì˜ˆ: í† í° ì„ë² ë”©)ì„ ë°›ì•„
â€œë‹¨ì–´ ê°„ì˜ ì—°ê´€ì„±(Attention)â€ì„ ê³„ì‚°í•˜ê³ ,
ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì„ë² ë”© í˜•íƒœë¡œ ëŒë ¤ì£¼ëŠ” ëª¨ë“ˆ

## í´ë˜ìŠ¤ ì •ì˜ ë° ì´ˆê¸°í™” ë¶€ë¬¸
### ì¸ì
``` ì¸ì
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
```
- `d_in`: ì…ë ¥ ì°¨ì› (ì…ë ¥ ë²¡í„°ì˜ feature ìˆ˜)
- `d_out`: ì¶œë ¥ ì°¨ì› (ì „ì²´ í—¤ë“œë“¤ì„ í•©ì¹œ ì°¨ì›)
- `super().__init__()`ë¡œ `nn.Module` ì´ˆê¸°í™”.

> nn.Moduleì´ ë­”ë°?
>- nn.Moduleì€ PyTorchì—ì„œ ì‹ ê²½ë§(Neural Network)ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•˜ëŠ” **ëª¨ë“  ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤(base class)**
### í—¤ë“œ ê°œìˆ˜ í• ë‹¹
``` 
assert d_out % NUM_HEADS == 0, "d_out must be divisible by n_heads"
```
- d_outì´ NUM_HEADSë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸. ê° í—¤ë“œì˜ ì°¨ì›(head_dim) = d_out // NUM_HEADS.
- d_outì€ ì¶œë ¥ ì°¨ì› ìˆ˜ (ì˜ˆ: 512)
- NUM_HEADSëŠ” ì–´í…ì…˜ í—¤ë“œì˜ ê°œìˆ˜ (ì˜ˆ: 8)
- ê° í—¤ë“œê°€ ì²˜ë¦¬í•  ì°¨ì›ì„ ë™ì¼í•˜ê²Œ ë‚˜ëˆ„ê¸° ìœ„í•´ d_outì´ NUM_HEADSë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨
- ì»´í“¨íŒ… ìì› ì—¬ìœ ê°€ ìˆìœ¼ë©´ í¬ê²Œ.

### Dimension
``` dim
self.d_out = d_out
self.head_dim = d_out // NUM_HEADS
```
- ë‚´ë¶€ ì €ì¥. `head_dim`ì€ í•œ í—¤ë“œê°€ ê°€ì§€ëŠ” feature ìˆ˜
- í•˜ë‚˜ì˜ ì–´í…ì…˜ í—¤ë“œê°€ ë‹´ë‹¹í•˜ëŠ” ì°¨ì› í¬ê¸°
- ì˜ˆ: d_out=512, NUM_HEADS=8 â†’ head_dim=64

### QKV ê°€ì¤‘ì¹˜ í–‰ë ¬
``` Weight Matrix
self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)
self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)
self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)
```
- ì…ë ¥ ë²¡í„° xë¥¼ ê°ê° Query, Key, Valueë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë³€í™˜ (ê°€ì¤‘ì¹˜ í–‰ë ¬).
- ì¦‰, ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ 3ê°€ì§€ ì—­í• ë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •.
- `QKV_BIAS`ëŠ” ì „ì—­ ìƒìˆ˜ë¡œ bias ì‚¬ìš© ì—¬ë¶€(ì°¸/ê±°ì§“)

### ì„ í˜• 
``` out_proj, Dropout
self.out_proj = nn.Linear(d_out, d_out)
self.dropout = nn.Dropout(DROP_RATE)
```
- ì—¬ëŸ¬ í—¤ë“œì˜ ì¶œë ¥ì„ ë‹¤ì‹œ í•©ì³ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ / ì—¬ëŸ¬ í—¤ë“œë¥¼ í•©ì¹œ ê²°ê³¼ì—
ë§ˆì§€ë§‰ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì¶œë ¥ íˆ¬ì‚¬ì¸µ 
- Dropoutìœ¼ë¡œ ì¼ë¶€ ì—°ê²°ì„ ëŠì–´ ê³¼ì í•© ë°©ì§€(regularization)
> regularizationì´ ì™œ í•„ìš”í•œë°? `d_model`ì´ ë¬´ìŠ¨ ì˜ë¯¸ì¸ë°?

### 
``` 
self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))
```
- ìƒì‚¼ê°í–‰ë ¬(ìœ„ìª½ì´ 1, ì•„ë˜ëŠ” 0)
- â€œë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œâ€ í•˜ëŠ” ìºì£¼ì–¼ ë§ˆìŠ¤í¬ (causal mask) (GPT ê°™ì€ ì–¸ì–´ëª¨ë¸ì—ì„œ ì¤‘ìš”)

ì˜ˆì‹œ (ê¸¸ì´ 4):
[[0, 1, 1, 1],
 [0, 0, 1, 1],
 [0, 0, 0, 1],
 [0, 0, 0, 0]]

## ìˆœì „íŒŒ (foward)
(1) ì…ë ¥ í˜•íƒœ
b, num_tokens, d_in = x.shape


b: batch size

num_tokens: í•œ ë¬¸ì¥(ì‹œí€€ìŠ¤)ì˜ í† í° ìˆ˜

d_in: ì…ë ¥ ì„ë² ë”© ì°¨ì›

ì˜ˆ: (b=16, num_tokens=128, d_in=512)

(2) Q, K, V ê³„ì‚°
keys = self.W_key(x)
queries = self.W_query(x)
values = self.W_value(x)


ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼
Query, Key, Value ë²¡í„°ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
shape: (b, num_tokens, d_out)

(3) ì—¬ëŸ¬ í—¤ë“œë¡œ ë¶„ë¦¬
keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)
queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)
values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)


ì´ì œ ê° ë‹¨ì–´ì˜ ì„ë² ë”©ì„ NUM_HEADS ê°œë¡œ ìª¼ê°­ë‹ˆë‹¤.

ì›ë˜: (b, num_tokens, 512)

ë°”ë€ í›„: (b, num_tokens, 8, 64)

(4) ì°¨ì› ì¬ë°°ì—´ (í—¤ë“œ ê¸°ì¤€ ê³„ì‚°í•˜ê¸° ìœ„í•´)
keys = keys.transpose(1, 2)
queries = queries.transpose(1, 2)
values = values.transpose(1, 2)


â†’ (b, NUM_HEADS, num_tokens, head_dim)

ì´ì œ ê° í—¤ë“œë³„ë¡œ ì–´í…ì…˜ ê³„ì‚° ê°€ëŠ¥.

(5) ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
attn_scores = queries @ keys.transpose(2, 3)


ì´ ì—°ì‚°ì€ Q Ã— Káµ€ (í–‰ë ¬ ê³±) ì…ë‹ˆë‹¤.

ê° í† í°ì´ ë‹¤ë¥¸ í† í°ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„.

shape: (b, NUM_HEADS, num_tokens, num_tokens)

(6) ë§ˆìŠ¤í¬ ì ìš© (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
attn_scores.masked_fill_(mask_bool, -torch.inf)


ë§ˆìŠ¤í¬ê°€ 1ì¸ ìœ„ì¹˜(=ë¯¸ë˜)ëŠ” -infë¡œ ì±„ì›Œ
softmax í›„ 0ì´ ë˜ê²Œ í•¨.

ì¦‰, í˜„ì¬ ë‹¨ì–´ëŠ” ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ.

(7) Softmaxë¡œ ê°€ì¤‘ì¹˜ ë³€í™˜
```
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
```
ìŠ¤ì½”ì–´ë¥¼ ì •ê·œí™”í•˜ì—¬ í™•ë¥ ì²˜ëŸ¼ ë§Œë“­ë‹ˆë‹¤.

âˆšdâ‚–ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ : í° ì°¨ì›ì¼ìˆ˜ë¡ ë‚´ì ê°’ì´ ì»¤ì ¸
softmaxê°€ saturation ë˜ëŠ” ê±¸ ë°©ì§€.

(8) Valueë¥¼ ê°€ì¤‘í•©
```
context_vec = (attn_weights @ values).transpose(1, 2)
```

ê° í† í°ì˜ â€œë¬¸ë§¥ ì •ë³´â€ë¥¼ ê³„ì‚°.

(b, num_tokens, NUM_HEADS, head_dim)

(9) ì—¬ëŸ¬ í—¤ë“œ ê²°ê³¼ í•©ì¹˜ê¸°
```
context_vec = context_vec.reshape(b, num_tokens, self.d_out)
context_vec = self.out_proj(context_vec)
```

í—¤ë“œë³„ ì¶œë ¥ì„ í•©ì³ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë˜ëŒë¦¼.

ìµœì¢… ì¶œë ¥ shape: (b, num_tokens, d_out)